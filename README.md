# FUREcast – SPYM (Formerly SPLG) Prediction & Analytics Platform

Hosted on Streamlit Cloud: https://splg-pulse.streamlit.app/
---

FUREcast is an educational analytics platform for the SPYM ETF (formerly SPLG) combining a Gradient Boosting Regressor (GBR) prediction model, engineered technical features, automated nightly data updates, evaluation tooling, and agentic natural-language interaction. While suitable for academic exploration, outputs (including predictions) are not investment advice.

### Migration Notice (SPLG → SPYM)
The underlying ETF formerly traded under the SPLG ticker. Key migration dates:
- 2025-10-31: Last date SPLG data was broadly available prior to deprecation.
- 2025-11-21: Yahoo Finance (yfinance) ceased returning fresh SPLG daily bars (stale dataset ending 2025-11-21).
- 2025-11-24: FEURCast production data pipeline switched to the SPYM ticker exclusively.

Historical CSV filenames (`SPLG_history_full.csv`, `rich_features_SPLG_history_full.csv`) are retained for backward compatibility; rows after 2025-11-21 originate from SPYM. Any references to “SPLG” in legacy function names (e.g. `predict_splg`) remain for stability and will be refactored later.

---

## 1. Core Objectives
- Predict next‑day SPYM percentage return (`target_return_t1`) using historical SPLG+SPYM merged timeline
- Surface interpretable drivers via feature importance
- Provide exploratory visualizations of historical price, sector composition, and risk
- Enable natural-language querying over model outputs and data
- Maintain reproducibility with documented, automated retraining
- Benchmark against naive baselines (previous-day return, zero, mean)

---

## 2. High-Level Architecture

```
Data (SPYM OHLCV; legacy SPLG historical retained) → Feature Engineering → GBR Training → Artifacts (models/)
  │                                       │                    │
  └── Nightly Update Workflow (GitHub Action) ──> Streamlit App + Agent
```

Components:
- Streamlit application: [streamlit/production/app.py](streamlit/production/app.py)
- Prediction & feature extraction: [streamlit/production/pred_model/predict.py](streamlit/production/pred_model/predict.py), [streamlit/production/pred_model/get_latest_features.py](streamlit/production/pred_model/get_latest_features.py)
- Automated update & retrain pipeline: [streamlit/production/pred_model/update_and_retrain.py](streamlit/production/pred_model/update_and_retrain.py)
- Feature engineering: [streamlit/production/pred_model/build_splg_features.py](streamlit/production/pred_model/build_splg_features.py)
- Incremental feature rebuild: [streamlit/production/pred_model/feature_updater.py](streamlit/production/pred_model/feature_updater.py)
- Raw & engineered datasets (legacy SPLG filenames, now appending SPYM): [data/SPLG_history_full.csv](data/SPLG_history_full.csv), [data/rich_features_SPLG_history_full.csv](data/rich_features_SPLG_history_full.csv)
- Model training & evaluation: [streamlit/production/pred_model/scripts/train_gbr_model.py](streamlit/production/pred_model/scripts/train_gbr_model.py), [streamlit/production/pred_model/scripts/evaluate_model.py](streamlit/production/pred_model/scripts/evaluate_model.py)
- Agentic interface: [streamlit/production/llm_interface.py](streamlit/production/llm_interface.py)
- Simulation & fallback logic: [streamlit/production/simulator.py](streamlit/production/simulator.py)

---

## 3. Data Sources & Management

### 3.1 Primary Prediction Dataset
- File: [data/rich_features_SPLG_history_full.csv](data/rich_features_SPLG_history_full.csv) (SPLG through 2025-11-21, SPYM thereafter)
- Range: 2006-09-01 → 2025-09-24 (≈19 years, 4,748 rows)
- Target variables:
  - `target_return_t1`: next-day percentage return (primary)
  - `target_close_t1`: next-day close (secondary / optional)
- Generated by feature pipeline: [`engineer_features`](streamlit/production/pred_model/build_splg_features.py)

### 3.2 Raw Historical Data
- File: [data/SPLG_history_full.csv](data/SPLG_history_full.csv) (legacy name)
- Source Acquisition: yfinance (SPLG until 2025-11-21; SPYM from 2025-11-24 onward via [`data_updater`](streamlit/production/pred_model/data_updater.py))
- Update Trigger: Nightly GitHub Action (workflow: [.github/workflows/daily-retrain.yml](.github/workflows/daily-retrain.yml))

### 3.3 Feature Engineering Overview
Feature categories (not exhaustive):
1. Price & Volume: OHLCV, spreads, normalizations
2. Price Relationships: ratios, differences (close/open, high/low)
3. Returns: rolling horizon simple & log returns (1d, 5d, 10d, 20d)
4. Trend / Moving Averages: multiple window MAs, slopes, ratios
5. Momentum: RSI, MACD components, ROC, raw momentum
6. Volatility: rolling std/var, ATR proxies, band widths
7. Volume Analytics: moving averages, volume ratios, OBV-like constructs
8. Lags: autoregressive lag features (1–10 day offsets, extended set)
9. Rolling Statistics: rolling min/max/skew/kurtosis windows
10. Calendar: day-of-week, month, quarter, year encodings

NaN Handling Strategies (in training script):
- Drop columns >95% NaN
- Forward-fill rolling window gaps (early periods)
- Replace infinities after division with safe values
- Preserve chronological order (no shuffling) to prevent look-ahead bias

### 3.4 Visualization Data
- Price history, engineered features, and simulated sector aggregates
- Sector holdings & mapping: [data/holdings-with-sectors.csv](data/holdings-with-sectors.csv), [data/treemap_nodes.csv](data/treemap_nodes.csv)
- Crisis timeline / events: [data/Financial Crisis Timeline by Day (since 2005).csv](data/Financial Crisis Timeline by Day (since 2005).csv)

### 3.5 Storage & Versioning
- Model artifacts: [streamlit/production/pred_model/models/](streamlit/production/pred_model/models/) (created post-training)
- Plots: [streamlit/production/pred_model/plots/](streamlit/production/pred_model/plots/)
- Logs: [streamlit/production/pred_model/logs/](streamlit/production/pred_model/logs/)
- Metadata: `model_metadata.json` (hyperparameters, data hash, timestamp)

---

## 4. Prediction Model (GBR)

### 4.1 Objective
Predict `target_return_t1` (next-day percentage return) using GradientBoostingRegressor with engineered technical, volatility, and calendar features.

### 4.2 Training Pipeline
See `train_gbr_model.py` (chronological split, scaling, training, metrics, persistence).

### 4.3 Quick Mode Hyperparameters (Updated)
```
n_estimators: 1200
learning_rate: 0.01
max_depth: 5
min_samples_split: 20
min_samples_leaf: 5
subsample: 0.7
max_features: "sqrt"
validation_fraction: 0.10
n_iter_no_change: 50
tol: 1e-4
random_state: 42
```
If a previous tuned model exists, quick mode reuses its best parameters.

### 4.4 Evaluation Metrics (Updated)
We removed raw MAPE (unstable with near‑zero daily returns) and added sMAPE plus naive baselines.

Stored per set (train / validation / test):
- MSE, RMSE, MAE, R²
- sMAPE (symmetric MAPE; robust near zero)
- Directional Accuracy (sign correctness)
- Predictions sample (capped for large sets)

Financial metrics (computed in `evaluate_model.py`):
- Sharpe Ratio (annualized; excess return assumes 2% risk-free)
- Max Drawdown (on cumulative strategy equity curve)
- Profit Factor (sum wins / sum losses)
- Win Rate, Avg Win, Avg Loss

Strategy logic: go long if prediction > 0 else short; strategy return = sign(prediction) * actual return.

### 4.5 Baseline Comparisons (New)
`metrics.json` now includes:
```
"baselines": {
  "lag": { "train": {...}, "val": {...}, "test": {...} },
  "zero": { "val": {...}, "test": {...} },
  "mean": { "val": {...}, "test": {...} }
}
```
Baselines:
- lag: previous day’s actual return (shifted)  
- zero: always 0% prediction  
- mean: training set average return  

Fields mirror model metrics (mse, rmse, mae, r2, smape, directional_accuracy).

Interpretation guide:
- Improvement (MAE): (MAE_baseline − MAE_model) / MAE_baseline  
- Directional lift: DirectionalAccuracy_model − max(DirectionalAccuracy_baselines)  
- R² vs baselines: any positive R² when baselines show near-zero is incremental predictive signal.

Example check (Python):
```python
m = json.load(open("models/metrics.json"))
mae_model = m["test"]["mae"]
mae_mean = m["baselines"]["mean"]["test"]["mae"]
print("MAE improvement over mean: {:.2%}".format((mae_mean - mae_model)/mae_mean))
```

Success threshold (suggested):
- Test MAE improvement > 2–5% over mean baseline
- Directional Accuracy ≥ 52% (vs random ≈50%)
- Stable Sharpe > 0.5 with realistic drawdowns (avoid artifacts)

### 4.6 Training Progress Plot
`evaluate_model.py` exports `training_progress.png`:
- Curve = `model.train_score_` per boosting iteration
- Labeled “Training Deviance (MSE)” (for squared_error loss deviance = MSE)
- Healthy learning shows monotonic or diminishing drops; flat/oscillatory early plateau suggests underfitting or insufficient learning rate / tree depth tuning.

### 4.7 Feature Importance
Top features saved to `feature_importance.csv` and plotted (top 30). Use to prune low-importance features and iterate.

### 4.8 Failsafe & Fallback
If model artifacts are missing or load errors occur:
- [`predict_splg`](streamlit/production/simulator.py) falls back to LLM-generated simulated prediction
- Synthesizes plausible `predicted_return`, `direction`, confidence, and synthetic top features
- Ensures uninterrupted application operation

---

## 5. Automated Update & Nightly Retraining

Workflow Summary (triggered by GitHub Action [.github/workflows/daily-retrain.yml](.github/workflows/daily-retrain.yml)):
1. Fetch new SPYM data (legacy SPLG historical retained): [`check_for_updates`](streamlit/production/pred_model/data_updater.py)
2. Rebuild features incrementally or full: [`rebuild_features_from_scratch`](streamlit/production/pred_model/feature_updater.py)
3. Retrain (quick mode unless configured): [`main`](streamlit/production/pred_model/scripts/train_gbr_model.py)
4. Evaluate & update plots: [`main`](streamlit/production/pred_model/scripts/evaluate_model.py)
5. Log & append training history: [`log_training_results`](streamlit/production/pred_model/training_logger.py)

Manual orchestration:
```bash
cd streamlit/production/pred_model
python update_and_retrain.py --quick
```

---

## 6. Agentic Features (Natural Language Interface)

Module: [streamlit/production/llm_interface.py](streamlit/production/llm_interface.py)

Capabilities:
- Intent routing: [`route_query`](streamlit/production/llm_interface.py) (market_outlook, sector_analysis, feature_explanation, custom_visualization, general_question)
- Tool plan generation & response composition: [`compose_answer`](streamlit/production/llm_interface.py)
- Prediction explanation: [`explain_prediction`](streamlit/production/llm_interface.py)
- Access to:
  - Model predictions via [`predict_splg`](streamlit/production/simulator.py)
  - Historical prices via [`fetch_prices`](streamlit/production/simulator.py)
  - Risk metrics via [`compute_risk`](streamlit/production/simulator.py)
  - Visualization builders (e.g. sector treemaps, feature importance charts)

Underlying API: OpenAI (API key loaded from `.env` / secrets)

---

## 7. Application Hosting & Deployment

- Hosted on Streamlit Cloud: https://splg-pulse.streamlit.app/
- Entry point: [streamlit/production/app.py](streamlit/production/app.py)
- Modes:
  - Lite Mode: Core prediction + feature importance + charts
  - Pro Mode: Query interface (“Ask FUREcast”), expanded analytics panels
- Automatic detection of trained model vs fallback simulation

---

## 8. Repository Structure (Condensed)

```
/
├── README.md                  # (This file)
├── requirements.txt           # Global dependencies
├── data/                      # Raw & engineered SPLG datasets
├── wrangling/                 # Feature wrangling/engineering docs & notebooks
│   |── pred_model_feature_engineering/
│   |   └── README.md
│   └── sector_performance_treemap
│       └── SPLG_holdings_sector_fill
├── streamlit/
│   └── production/
│       ├── app.py
│       ├── simulator.py
│       ├── llm_interface.py
│       └── pred_model/
│           ├── update_and_retrain.py
│           ├── data_updater.py
│           ├── feature_updater.py
│           ├── scripts/
│           │   ├── train_gbr_model.py
│           │   └── evaluate_model.py
│           ├── predict.py
│           ├── get_latest_features.py
│           ├── models/ (generated during training)
│           ├── plots/  (generated during evaluation)
│           ├── logs/   (generated during training and evaluation)
├── .github/workflows/daily-retrain.yml
└── .env / .streamlit.secrets.toml
```

---

## 9. Setup & Installation

### 9.1 Prerequisites
- Python 3.11+
- (Recommended) Virtual environment (venv or conda)
- OpenAI API key (for agentic features & fallback)

### 9.2 Install Dependencies
```bash
python -m venv .venv
source .venv/bin/activate        # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```
Dependencies file: [requirements.txt](requirements.txt)

### 9.3 Environment Configuration
Create `.env`:
```
OPENAI_API_KEY=sk-...(can be provided upon request)
MARKETSENTIMENT_API_KEY = "d46lnphr01qgc9etei60d46lnphr01qgc9etei6g"
MARKET_AUX_API_KEY = "sfvJUu6JVVkhI6YucUlZeGTEwHkzic26DTwmnJ51"
FRED_KEY = "167c610d0808df0df6fc03d8a7c9f611"
```
Streamlit Cloud: use `.streamlit.secrets.toml` (not versioned with key content)

---

## 10. Running the Application

From repository root:
```bash
cd streamlit/production
streamlit run app.py
```

Optional flags (Streamlit standard) for headless/server deployment.

---

## 11. Training & Evaluation Workflows

### Quick Training & Complete Evaluation
```bash
cd streamlit/production/pred_model
python scripts/train_gbr_model.py --quick
python scripts/evaluate_model.py
```

### Full Tuned Training (updates optimal hyperparameters)
Performs GridSearch with TimeSeriesSplit; results stored and reused by subsequent `--quick`.
```bash
python scripts/train_gbr_model.py --tune
```

### Orchestrated End-to-End Training & Evaluation (Data update → features → train → evaluate)
```bash
python update_and_retrain.py --tune       # full tuning workflow
python update_and_retrain.py --quick      # faster retrain
python update_and_retrain.py --force      # retrain even if no new data
```

### Automated Nightly Data Update & Model Retraining
Workflow: [.github/workflows/daily-retrain.yml](.github/workflows/daily-retrain.yml) → [`update_and_retrain.py`](streamlit/production/pred_model/update_and_retrain.py)

---


5. If model fails improvement thresholds, rerun with `--tune` or revise features.

## 12. Metrics & Interpretation

Stored in: `models/metrics.json` (post-training)
- Use charts in `plots/` for qualitative diagnostics
- Directional accuracy above random baseline (≈>52%) treated as a success threshold
- Low or negative R² common in noisy daily return prediction—emphasis on stability & calibration

Explanation:
- Feature importance from tree-based splits (`model.feature_importances_`)
- Natural language summarization via [`explain_prediction`](streamlit/production/llm_interface.py)


### Baseline Comparison Procedure
1. Run training (quick or tune) then evaluation.
2. Open `models/metrics.json`.
3. Compare:
   - Model MAE vs mean + zero baselines.
   - Directional accuracy vs lag baseline.
   - sMAPE vs baselines (lower is better).
4. Review `feature_importance.csv` to ensure diversified signal.

---

## 13. Fallback & Robustness

If model artifacts missing or invalid:
- [`predict_splg`](streamlit/production/simulator.py) synthesizes plausible prediction & feature set
- UI continues functioning (cards, charts, agent interface)
- Console/log prompts user to retrain (`train_gbr_model.py --quick`)

---

## 14. Logging & Monitoring

Locations:
- Training logs: [streamlit/production/pred_model/logs/](streamlit/production/pred_model/logs/)
- Evaluation logs: same directory (timestamped)
- Update flow logs aggregated by nightly automation
- History tracking: [training_history.jsonl](streamlit/production/pred_model/logs/training_history.jsonl)

---

## 15. Reproducibility & Versioning

Artifacts:
- `model_metadata.json`: hyperparameters, timestamp, feature count, data hash
- Chronological splits ensure consistency
- Random seed: 42 (model + NumPy)
- No data shuffling; all features derived from past-only windows

Documentation References:
- [wrangling/pred_model_feature_engineering/GBR_MODEL_TRAINING_GUIDE.md](wrangling/pred_model_feature_engineering/GBR_MODEL_TRAINING_GUIDE.md)
- [wrangling/pred_model_feature_engineering/QUICK_START_MODEL_TRAINING.md](wrangling/pred_model_feature_engineering/QUICK_START_MODEL_TRAINING.md)
- [streamlit/production/pred_model/IMPLEMENTATION_SUMMARY.md](streamlit/production/pred_model/IMPLEMENTATION_SUMMARY.md)

---

## 16. Security & Secrets

- API keys loaded via `.env` or Streamlit secrets
- No secrets committed to version control
- Defensive error handling around missing keys in:
  - [`get_openai_client`](streamlit/production/llm_interface.py)
  - Fallback simulation in [`predict_splg`](streamlit/production/simulator.py)

---

## 17. Roadmap (Potential Enhancements)

- Model ensembling (e.g., XGBoost / LightGBM comparison)
- Probabilistic calibration (quantile regression / conformal prediction)
- Extended sector / macro factor integration
- Live feature delta monitoring
- Adaptive retraining triggers (performance drift detection)

---

## 18. Support & Troubleshooting

Common Issues:
- Model not found → Retrain: `python scripts/train_gbr_model.py --quick`
- Low R² / accuracy → Consider tuning (`python scripts/train_gbr_model.py --tune`)
- Fallback triggered repeatedly → Check logs, confirm artifacts in `models/`

Reference Files:
- Training: [streamlit/production/pred_model/scripts/train_gbr_model.py](streamlit/production/pred_model/scripts/train_gbr_model.py)
- Evaluation: [streamlit/production/pred_model/scripts/evaluate_model.py](streamlit/production/pred_model/scripts/evaluate_model.py)
- Prediction: [streamlit/production/pred_model/predict.py](streamlit/production/pred_model/predict.py)

---

## 19. Academic Context

Extended methodological and feature documentation located in:
- [wrangling/pred_model_feature_engineering/README.md](wrangling/pred_model_feature_engineering/README.md)
- [wrangling/pred_model_feature_engineering/GBR_MODEL_TRAINING_GUIDE.md](wrangling/pred_model_feature_engineering/GBR_MODEL_TRAINING_GUIDE.md)
- Data dictionary (feature catalog) in `DATA_DICTIONARY.md` within same directory (not excerpted here).

---

## 20. License & Usage

Educational project—consult maintainers for usage clarification. Not financial advice.

---

## 21. Quick Start Summary

```bash
# 1. Clone & enter
git clone https://github.com/nthPerson/FEURCast.git
cd FEURCast

# 2. Environment
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# 3. (Optional) Train model
cd streamlit/production/pred_model
python scripts/train_gbr_model.py --quick
python scripts/evaluate_model.py

# 4. Run app
cd ../
streamlit run app.py
```

Fallback prediction may appear if step 3 is skipped.

---

For additional details, inspect:
- Execution flow: [`main`](streamlit/production/pred_model/update_and_retrain.py)
- Fallback logic: [`predict_splg`](streamlit/production/simulator.py)
- LLM orchestration: [`route_query`](streamlit/production/llm_interface.py), [`compose_answer`](streamlit/production/llm_interface.py)

---